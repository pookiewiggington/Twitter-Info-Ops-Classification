{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "preprocessRussianTweets.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIv4kNd324rb"
      },
      "source": [
        "<div align=\"center\"><h1><b>Step 1: PreProcess Russian Tweets</b></h1></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0pnZ3mApJeP"
      },
      "source": [
        "**Outline**\n",
        "\n",
        "1. Merge Russian Information Operations datasets from Twitter's public information operations datasets (https://transparency.twitter.com/en/reports/information-operations.html)\n",
        "\n",
        "2. Format datasets and ensure consistency between the users in the user and tweet data\n",
        "3. Perform lemmatization and preprocessing on the tweets to create a list of formatted words as an additional feature for every user and tweet. The user feature will be the aggregation of all their tweet BoWs.\n",
        "4. Create a list of all BoWs to query legitimate users in Step 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWwyl6zm1LYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2085b54f-dbf1-4b84-8805-cf0a3c3cb07a"
      },
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "# For accessing Google Drive Files\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, drive\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Connect and authenticate Google Drive with Google CoLab\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive.mount('/drive')\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# For NLP, Exploratory Data Analysis, Twitter API access\n",
        "import CS3315Project.tweetProcessing as tweetProcessing\n",
        "import pandas as pd\n",
        "import re\n",
        "import itertools\n",
        "import numpy as np "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /drive; to attempt to forcibly remount, call drive.mount(\"/drive\", force_remount=True).\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXcBQS28Oiaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7681adba-c4f6-4500-c6f0-97e89e2e260c"
      },
      "source": [
        "# Grab Russian disinformation tweet and user datasets from CSVs on Google Drive, convert to dataframes\n",
        "\n",
        "print('Accessing shared file links...')\n",
        "rustweetssep = drive.CreateFile({'id':'insert file id'}) \n",
        "rususerssep = drive.CreateFile({'id': 'insert file id'})\n",
        "rustweetsmay1 = drive.CreateFile({'id': 'insert file id'}) \n",
        "rustweetsmay2 = drive.CreateFile({'id': 'insert file id'}) \n",
        "rususerssmay = drive.CreateFile({'id': 'insert file id'}) \n",
        "rustweetsjun2019 = drive.CreateFile({'id': 'insert file id'})\n",
        "rususersjun2019 = drive.CreateFile({'id': 'insert file id'})\n",
        "rustweetsjan2019 = drive.CreateFile({'id': 'insert file id'})\n",
        "rususersjan2019 = drive.CreateFile({'id':'insert file id'})\n",
        "rustweetsoct2018 = drive.CreateFile({'id':'insert file id'})\n",
        "rususersoct2018 = drive.CreateFile({'id':'insert file id'})\n",
        "print('Links accessed')\n",
        "\n",
        "print('Getting the file contents...')\n",
        "rustweetssep.GetContentFile('ira_092020_tweets_csv_hashed.csv')\n",
        "rususerssep.GetContentFile('ira_092020_users_csv_hashed.csv')\n",
        "rustweetsmay1.GetContentFile('russia_052020_tweets_csv_hashed_1.csv')\n",
        "rustweetsmay2.GetContentFile('russia_052020_tweets_csv_hashed_2.csv')\n",
        "rususerssmay.GetContentFile('russia_052020_users_csv_hashed.csv')\n",
        "rustweetsjun2019.GetContentFile('russia_201906_1_tweets_csv_hashed.csv')\n",
        "rususersjun2019.GetContentFile('russia_201906_1_users_csv_hashed.csv')\n",
        "rustweetsjan2019.GetContentFile('russia_201901_linked_tweets_csv_hashed_201901_1.csv')\n",
        "rususersjan2019.GetContentFile('russia_201901_1_users_csv_hashed.csv')\n",
        "rustweetsoct2018.GetContentFile('ira_tweets_csv_hashed.csv')\n",
        "rususersoct2018.GetContentFile('ira_users_csv_hashed.csv')\n",
        "print('Files retrieved')\n",
        "\n",
        "print('Creating dataframes...')\n",
        "rtweets_0920_df1 = pd.read_csv('ira_092020_tweets_csv_hashed.csv')\n",
        "rtweets_0520_df2 = pd.read_csv('russia_052020_tweets_csv_hashed_1.csv')\n",
        "rtweets_0520_df3 = pd.read_csv('russia_052020_tweets_csv_hashed_2.csv')\n",
        "rtweets_0619_df4 = pd.read_csv('russia_201906_1_tweets_csv_hashed.csv')\n",
        "rtweets_0119_df5 = pd.read_csv('russia_201901_linked_tweets_csv_hashed_201901_1.csv')\n",
        "rtweets_1018_df6 = pd.read_csv('ira_tweets_csv_hashed.csv')\n",
        "\n",
        "rusers_0920_df1 = pd.read_csv('ira_092020_users_csv_hashed.csv')\n",
        "rusers_0520_df2 = pd.read_csv('russia_052020_users_csv_hashed.csv')\n",
        "rusers_0619_df3 = pd.read_csv('russia_201906_1_users_csv_hashed.csv')\n",
        "rusers_0119_df4 = pd.read_csv('russia_201901_1_users_csv_hashed.csv')\n",
        "rusers_1018_df5 = pd.read_csv('ira_users_csv_hashed.csv')\n",
        "print('Dataframes created')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accessing shared file links...\n",
            "Links accessed\n",
            "Getting the file contents...\n",
            "Files retrieved\n",
            "Creating dataframes...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (15,19) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n",
            "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (30) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  interactivity=interactivity, compiler=compiler, result=result)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Dataframes created\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDc76UKvajSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14857e74-70d7-4aeb-a440-a6aae025413e"
      },
      "source": [
        "# Look at shapes of the tweet datasets, ensure consistency\n",
        "\n",
        "print(rtweets_0920_df1.shape)\n",
        "print(rtweets_0520_df2.shape)\n",
        "print(rtweets_0520_df3.shape)\n",
        "print(rtweets_0619_df4.shape)\n",
        "print(rtweets_0119_df5.shape)\n",
        "print(rtweets_1018_df6.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1368, 30)\n",
            "(3128489, 30)\n",
            "(306303, 30)\n",
            "(3, 31)\n",
            "(920761, 31)\n",
            "(8768633, 31)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK2Za_NgQOMW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f5b21d-7860-47e6-eceb-3abd50acfb61"
      },
      "source": [
        "# Identify why there is an extra column in one dataset\n",
        "\n",
        "print(rtweets_0920_df1.info())\n",
        "print(rtweets_1018_df6.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1368 entries, 0 to 1367\n",
            "Data columns (total 30 columns):\n",
            " #   Column                    Non-Null Count  Dtype  \n",
            "---  ------                    --------------  -----  \n",
            " 0   tweetid                   1368 non-null   int64  \n",
            " 1   userid                    1368 non-null   object \n",
            " 2   user_display_name         1368 non-null   object \n",
            " 3   user_screen_name          1368 non-null   object \n",
            " 4   user_reported_location    919 non-null    object \n",
            " 5   user_profile_description  1368 non-null   object \n",
            " 6   user_profile_url          1368 non-null   object \n",
            " 7   follower_count            1368 non-null   int64  \n",
            " 8   following_count           1368 non-null   int64  \n",
            " 9   account_creation_date     1368 non-null   object \n",
            " 10  account_language          1368 non-null   object \n",
            " 11  tweet_language            1368 non-null   object \n",
            " 12  tweet_text                1368 non-null   object \n",
            " 13  tweet_time                1368 non-null   object \n",
            " 14  tweet_client_name         1368 non-null   object \n",
            " 15  in_reply_to_userid        72 non-null     object \n",
            " 16  in_reply_to_tweetid       72 non-null     float64\n",
            " 17  quoted_tweet_tweetid      36 non-null     float64\n",
            " 18  is_retweet                1368 non-null   bool   \n",
            " 19  retweet_userid            270 non-null    object \n",
            " 20  retweet_tweetid           494 non-null    float64\n",
            " 21  latitude                  1368 non-null   object \n",
            " 22  longitude                 1368 non-null   object \n",
            " 23  quote_count               1368 non-null   int64  \n",
            " 24  reply_count               1368 non-null   int64  \n",
            " 25  like_count                1368 non-null   int64  \n",
            " 26  retweet_count             1368 non-null   int64  \n",
            " 27  hashtags                  1368 non-null   object \n",
            " 28  urls                      1368 non-null   object \n",
            " 29  user_mentions             1368 non-null   object \n",
            "dtypes: bool(1), float64(3), int64(7), object(19)\n",
            "memory usage: 311.4+ KB\n",
            "None\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8768633 entries, 0 to 8768632\n",
            "Data columns (total 31 columns):\n",
            " #   Column                    Dtype  \n",
            "---  ------                    -----  \n",
            " 0   tweetid                   int64  \n",
            " 1   userid                    object \n",
            " 2   user_display_name         object \n",
            " 3   user_screen_name          object \n",
            " 4   user_reported_location    object \n",
            " 5   user_profile_description  object \n",
            " 6   user_profile_url          object \n",
            " 7   follower_count            int64  \n",
            " 8   following_count           int64  \n",
            " 9   account_creation_date     object \n",
            " 10  account_language          object \n",
            " 11  tweet_language            object \n",
            " 12  tweet_text                object \n",
            " 13  tweet_time                object \n",
            " 14  tweet_client_name         object \n",
            " 15  in_reply_to_tweetid       float64\n",
            " 16  in_reply_to_userid        object \n",
            " 17  quoted_tweet_tweetid      float64\n",
            " 18  is_retweet                bool   \n",
            " 19  retweet_userid            object \n",
            " 20  retweet_tweetid           float64\n",
            " 21  latitude                  object \n",
            " 22  longitude                 object \n",
            " 23  quote_count               float64\n",
            " 24  reply_count               float64\n",
            " 25  like_count                float64\n",
            " 26  retweet_count             float64\n",
            " 27  hashtags                  object \n",
            " 28  urls                      object \n",
            " 29  user_mentions             object \n",
            " 30  poll_choices              object \n",
            "dtypes: bool(1), float64(7), int64(3), object(20)\n",
            "memory usage: 2.0+ GB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PT1Ge21Qn26"
      },
      "source": [
        "The earlier tweet datasets contain an extra column 'poll_choices', that is not available in the later datasets, so we will drop that column. This column is intended to indicate the choices available if someone posted a poll, so it is not a necessary feature for our purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K15QF2ifQvSq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ccfaddf-c1d2-4606-93fb-8a54166fd34a"
      },
      "source": [
        "# Drop poll_choices from earliest datasets\n",
        "\n",
        "rtweets_0619_df4 = rtweets_0619_df4.drop(['poll_choices'], axis=1)\n",
        "rtweets_0119_df5 = rtweets_0119_df5.drop(['poll_choices'], axis=1)\n",
        "rtweets_1018_df6 = rtweets_1018_df6.drop(['poll_choices'], axis=1)\n",
        "\n",
        "print(rtweets_0619_df4.shape)\n",
        "print(rtweets_0119_df5.shape)\n",
        "print(rtweets_1018_df6.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3, 30)\n",
            "(920761, 30)\n",
            "(8768633, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEMbrORp-oPX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f590cd2-e2b7-4909-cf22-5ab0a85dccb4"
      },
      "source": [
        "# Look at shapes of the user datasets, ensure consistency\n",
        "\n",
        "print(rusers_0920_df1.shape)\n",
        "print(rusers_0520_df2.shape)\n",
        "print(rusers_0619_df3.shape)\n",
        "print(rusers_0119_df4.shape)\n",
        "print(rusers_1018_df5.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5, 10)\n",
            "(1153, 10)\n",
            "(3, 10)\n",
            "(416, 10)\n",
            "(3608, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEQrcdXW-1Km"
      },
      "source": [
        "# Add columns annotating each dataset in order to differentiate them\n",
        "\n",
        "rtweets_0920_df1['dataset'] = '0920'\n",
        "rusers_0920_df1['dataset'] = '0920'\n",
        "\n",
        "rtweets_0520_df2['dataset'] = '0520'\n",
        "rtweets_0520_df3['dataset'] = '0520'\n",
        "rusers_0520_df2['dataset'] = '0520'\n",
        "\n",
        "rtweets_0619_df4['dataset'] = '0619'\n",
        "rusers_0619_df3['dataset'] = '0619'\n",
        "\n",
        "rtweets_0119_df5['dataset'] = '0119'\n",
        "rusers_0119_df4['dataset'] = '0119'\n",
        "\n",
        "rtweets_1018_df6['dataset'] = '1018'\n",
        "rusers_1018_df5['dataset'] = '1018'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPxOqJXtSBXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89bcabee-b79b-4a2c-bd76-0ad5587a566c"
      },
      "source": [
        "# Verify column has been added\n",
        "\n",
        "print(rtweets_0920_df1.shape)\n",
        "print(rtweets_0520_df2.shape)\n",
        "print(rtweets_0520_df3.shape)\n",
        "print(rtweets_0619_df4.shape)\n",
        "print(rtweets_0119_df5.shape)\n",
        "print(rtweets_1018_df6.shape)\n",
        "print(rusers_0920_df1.shape)\n",
        "print(rusers_0520_df2.shape)\n",
        "print(rusers_0619_df3.shape)\n",
        "print(rusers_0119_df4.shape)\n",
        "print(rusers_1018_df5.shape)\n",
        "\n",
        "print(rtweets_0920_df1.head())\n",
        "print(rusers_0920_df1.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1368, 31)\n",
            "(3128489, 31)\n",
            "(306303, 31)\n",
            "(3, 31)\n",
            "(920761, 31)\n",
            "(8768633, 31)\n",
            "(5, 11)\n",
            "(1153, 11)\n",
            "(3, 11)\n",
            "(416, 11)\n",
            "(3608, 11)\n",
            "               tweetid  ... dataset\n",
            "0  1290351045160448005  ...    0920\n",
            "1  1268235122131771392  ...    0920\n",
            "2  1283019246503694336  ...    0920\n",
            "3  1273537153893629952  ...    0920\n",
            "4  1273195539383889921  ...    0920\n",
            "\n",
            "[5 rows x 31 columns]\n",
            "                                         userid  ... dataset\n",
            "0  CqW9bECdw2Jjk9DDU7UyE6P59TukYFISNE8J6sN66u4=  ...    0920\n",
            "1    uOrf1TDmM7vP4YEhOJDXORoqvpDlsJt03AyOfhrZo=  ...    0920\n",
            "2   LXW4uuq2JWx4So6ycDFanp4qYQxNvj0ftiuyUe3tZo=  ...    0920\n",
            "3   oqEFFiOrA+QVN8mEK0wweRTMmY2FQNB6XE5baB1Wik=  ...    0920\n",
            "4   KjTkk0ZTF6mmlwxdxA13V1UVlB+NAeaWoH9YqBCFEE=  ...    0920\n",
            "\n",
            "[5 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUCgLHty5agy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81c00b57-8db8-43ad-9c39-b9308f33ecc5"
      },
      "source": [
        "# Merge Russian tweet datasets\n",
        "\n",
        "rustw_dflist = [rtweets_0920_df1, rtweets_0520_df2, rtweets_0520_df3, rtweets_0619_df4, rtweets_0119_df5, rtweets_1018_df6]\n",
        "rus_tweetsdf_mrg = pd.concat(rustw_dflist)\n",
        "\n",
        "del rtweets_0920_df1, rtweets_0520_df2, rtweets_0520_df3, rtweets_0619_df4, rtweets_0119_df5, rtweets_1018_df6\n",
        "del rustw_dflist\n",
        "\n",
        "rus_tweetsdf_mrg.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 13125557 entries, 0 to 8768632\n",
            "Data columns (total 31 columns):\n",
            " #   Column                    Dtype  \n",
            "---  ------                    -----  \n",
            " 0   tweetid                   int64  \n",
            " 1   userid                    object \n",
            " 2   user_display_name         object \n",
            " 3   user_screen_name          object \n",
            " 4   user_reported_location    object \n",
            " 5   user_profile_description  object \n",
            " 6   user_profile_url          object \n",
            " 7   follower_count            int64  \n",
            " 8   following_count           int64  \n",
            " 9   account_creation_date     object \n",
            " 10  account_language          object \n",
            " 11  tweet_language            object \n",
            " 12  tweet_text                object \n",
            " 13  tweet_time                object \n",
            " 14  tweet_client_name         object \n",
            " 15  in_reply_to_userid        object \n",
            " 16  in_reply_to_tweetid       float64\n",
            " 17  quoted_tweet_tweetid      float64\n",
            " 18  is_retweet                bool   \n",
            " 19  retweet_userid            object \n",
            " 20  retweet_tweetid           float64\n",
            " 21  latitude                  object \n",
            " 22  longitude                 object \n",
            " 23  quote_count               float64\n",
            " 24  reply_count               float64\n",
            " 25  like_count                float64\n",
            " 26  retweet_count             float64\n",
            " 27  hashtags                  object \n",
            " 28  urls                      object \n",
            " 29  user_mentions             object \n",
            " 30  dataset                   object \n",
            "dtypes: bool(1), float64(7), int64(3), object(20)\n",
            "memory usage: 3.0+ GB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBnCDMdh5n1j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd444dc-9500-430e-afc9-575e700e55d8"
      },
      "source": [
        "# Merge Russian user datasets\n",
        "\n",
        "rusus_dflist = [rusers_0920_df1, rusers_0520_df2, rusers_0619_df3, rusers_0119_df4, rusers_1018_df5]\n",
        "rus_usersdf_mrg = pd.concat(rusus_dflist)\n",
        "\n",
        "del rusers_0920_df1, rusers_0520_df2, rusers_0619_df3, rusers_0119_df4, rusers_1018_df5\n",
        "del rusus_dflist\n",
        "\n",
        "rus_usersdf_mrg.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 5185 entries, 0 to 3607\n",
            "Data columns (total 11 columns):\n",
            " #   Column                    Non-Null Count  Dtype \n",
            "---  ------                    --------------  ----- \n",
            " 0   userid                    5185 non-null   object\n",
            " 1   user_display_name         5185 non-null   object\n",
            " 2   user_screen_name          5185 non-null   object\n",
            " 3   user_reported_location    3790 non-null   object\n",
            " 4   user_profile_description  3476 non-null   object\n",
            " 5   user_profile_url          376 non-null    object\n",
            " 6   follower_count            5152 non-null   object\n",
            " 7   following_count           5153 non-null   object\n",
            " 8   account_creation_date     5185 non-null   object\n",
            " 9   account_language          5185 non-null   object\n",
            " 10  dataset                   5185 non-null   object\n",
            "dtypes: object(11)\n",
            "memory usage: 486.1+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHRPdY5aU9vQ"
      },
      "source": [
        "There are 5,185 unique users in the Russian user dataset and 13,125,557 total tweets.\n",
        "\n",
        "Now that the data is merged, we need to drop rows with null values for user data or tweet text, so we can appropriately group by user ids and be preprocessed. We also need to ensure the two datasets are consistent, namely that all users in the user data have tweets and vice versa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET4Mhsx7by6M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "501a2a54-81c0-483a-f95a-a9b6c93ed7d2"
      },
      "source": [
        "# View null values in the user dataset to see if there are any unusable data points\n",
        "\n",
        "display(rus_usersdf_mrg.isnull().sum()) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "userid                         0\n",
              "user_display_name              0\n",
              "user_screen_name               0\n",
              "user_reported_location      1395\n",
              "user_profile_description    1709\n",
              "user_profile_url            4809\n",
              "follower_count                33\n",
              "following_count               32\n",
              "account_creation_date          0\n",
              "account_language               0\n",
              "dataset                        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh86Uunpb_5B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "c5f737f2-4092-4dd8-8b85-2befddeb3bf5"
      },
      "source": [
        "# View null values in the tweet dataset to see if there are any unusable data points\n",
        "\n",
        "display(rus_tweetsdf_mrg.isnull().sum()) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tweetid                            0\n",
              "userid                             0\n",
              "user_display_name                  0\n",
              "user_screen_name                   0\n",
              "user_reported_location       2894797\n",
              "user_profile_description     1859060\n",
              "user_profile_url             9916540\n",
              "follower_count                     0\n",
              "following_count                    0\n",
              "account_creation_date              0\n",
              "account_language                   0\n",
              "tweet_language                804083\n",
              "tweet_text                         2\n",
              "tweet_time                         0\n",
              "tweet_client_name              40341\n",
              "in_reply_to_userid          12031159\n",
              "in_reply_to_tweetid         12317236\n",
              "quoted_tweet_tweetid        12806654\n",
              "is_retweet                         0\n",
              "retweet_userid               9044607\n",
              "retweet_tweetid              8663898\n",
              "latitude                           0\n",
              "longitude                          0\n",
              "quote_count                     8708\n",
              "reply_count                     8708\n",
              "like_count                      8708\n",
              "retweet_count                   8708\n",
              "hashtags                     3323688\n",
              "urls                         2433398\n",
              "user_mentions                4785011\n",
              "dataset                            0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8syEfjX19mq"
      },
      "source": [
        "There are two rows where the tweet_text value is null, which may have negative effects, so we will drop these rows. There was nothing of concern in the user dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EqzzWEW1Bgs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "81cd7ac4-2b3f-4062-b5a9-1b9b9a6809f5"
      },
      "source": [
        "# Drop null rows for tweet_text column\n",
        "\n",
        "rus_tweetsdf_mrg = rus_tweetsdf_mrg.dropna(subset=['tweet_text'])\n",
        "\n",
        "display(rus_tweetsdf_mrg.isnull().sum()) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tweetid                            0\n",
              "userid                             0\n",
              "user_display_name                  0\n",
              "user_screen_name                   0\n",
              "user_reported_location       2894797\n",
              "user_profile_description     1859060\n",
              "user_profile_url             9916538\n",
              "follower_count                     0\n",
              "following_count                    0\n",
              "account_creation_date              0\n",
              "account_language                   0\n",
              "tweet_language                804083\n",
              "tweet_text                         0\n",
              "tweet_time                         0\n",
              "tweet_client_name              40341\n",
              "in_reply_to_userid          12031157\n",
              "in_reply_to_tweetid         12317234\n",
              "quoted_tweet_tweetid        12806652\n",
              "is_retweet                         0\n",
              "retweet_userid               9044605\n",
              "retweet_tweetid              8663896\n",
              "latitude                           0\n",
              "longitude                          0\n",
              "quote_count                     8708\n",
              "reply_count                     8708\n",
              "like_count                      8708\n",
              "retweet_count                   8708\n",
              "hashtags                     3323688\n",
              "urls                         2433398\n",
              "user_mentions                4785009\n",
              "dataset                            0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-4mmVDcoJHW"
      },
      "source": [
        "Now we can see there are no null values for tweet_text. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktgQ_s2AMLtW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17c43623-bb9d-422f-94a5-59d78842bcec"
      },
      "source": [
        "# Find number of users in each dataset\n",
        "tweetusers = rus_tweetsdf_mrg['userid'].unique()\n",
        "print('The number of users that tweeted in the Russian tweet dataset is ', len(tweetusers))\n",
        "print('The number of Russian users in the user dataset is ', len(rus_usersdf_mrg.index))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of users that tweeted in the Russian tweet dataset is  4861\n",
            "The number of Russian users in the user dataset is  5185\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xca08_MMziQ"
      },
      "source": [
        "The number of unique users between the tweet dataset and the user datasets do not match, so we will have to drop user columns that only have user info but do not contain corresponding tweets, because we cannot create word vectors for these users, which is the basis of our classification model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zdn6q4kgM94H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c31cd365-4000-4783-f6bf-9dd5787bb198"
      },
      "source": [
        "# Drop users that have no recorded tweets in the dataset\n",
        "\n",
        "# If user in tweet data set is in user dataset, return True, else return False\n",
        "rus_usersdf_mrg = rus_usersdf_mrg[rus_usersdf_mrg.userid.isin(tweetusers)]\n",
        "\n",
        "print('New number of users in the user dataset')\n",
        "print(len(rus_usersdf_mrg.index))\n",
        "print('Number of users in the tweet dataset')\n",
        "print(len(tweetusers))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "New number of users in the user dataset\n",
            "4859\n",
            "Number of users in the tweet dataset\n",
            "4861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfdNk8fFV_y_"
      },
      "source": [
        "It looks like there are two users that have tweets, but are not in the user dataset. Since all the user information in the user dataset is also in the tweet dataset, we can add these users to the user dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFkM92VXWLqJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "6f6b5fe7-e921-4409-cfc6-9ce9cfb5d8d3"
      },
      "source": [
        "# Identify two tweet users that aren't in the user dataset, grab one tweet from each\n",
        "\n",
        "extra_tweet_users = rus_tweetsdf_mrg[~rus_tweetsdf_mrg.userid.isin(rus_usersdf_mrg.userid)]\n",
        "extra_tweet_users = extra_tweet_users.drop_duplicates(subset = [\"userid\"])\n",
        "\n",
        "extra_tweet_users.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweetid</th>\n",
              "      <th>userid</th>\n",
              "      <th>user_display_name</th>\n",
              "      <th>user_screen_name</th>\n",
              "      <th>user_reported_location</th>\n",
              "      <th>user_profile_description</th>\n",
              "      <th>user_profile_url</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>account_creation_date</th>\n",
              "      <th>account_language</th>\n",
              "      <th>tweet_language</th>\n",
              "      <th>tweet_text</th>\n",
              "      <th>tweet_time</th>\n",
              "      <th>tweet_client_name</th>\n",
              "      <th>in_reply_to_userid</th>\n",
              "      <th>in_reply_to_tweetid</th>\n",
              "      <th>quoted_tweet_tweetid</th>\n",
              "      <th>is_retweet</th>\n",
              "      <th>retweet_userid</th>\n",
              "      <th>retweet_tweetid</th>\n",
              "      <th>latitude</th>\n",
              "      <th>longitude</th>\n",
              "      <th>quote_count</th>\n",
              "      <th>reply_count</th>\n",
              "      <th>like_count</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>hashtags</th>\n",
              "      <th>urls</th>\n",
              "      <th>user_mentions</th>\n",
              "      <th>dataset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>820382233420701697</td>\n",
              "      <td>iZ328VglWrG25qPym1bifLoiwXD9v1+A3G4WU5AThso=</td>\n",
              "      <td>iZ328VglWrG25qPym1bifLoiwXD9v1+A3G4WU5AThso=</td>\n",
              "      <td>iZ328VglWrG25qPym1bifLoiwXD9v1+A3G4WU5AThso=</td>\n",
              "      <td>United States</td>\n",
              "      <td>No more #HappyHolidays shit!!!\\nIt's #MerryChr...</td>\n",
              "      <td>https://t.co/XFnhCqCWBy</td>\n",
              "      <td>2718</td>\n",
              "      <td>264</td>\n",
              "      <td>2016-06-15</td>\n",
              "      <td>en</td>\n",
              "      <td>und</td>\n",
              "      <td>#RosieODonnellIsTrash #RosieThePig #Disgusting...</td>\n",
              "      <td>2017-01-14 21:29</td>\n",
              "      <td>Twitter for Android</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['RosieODonnellIsTrash', 'RosieThePig', 'Disgu...</td>\n",
              "      <td>[]</td>\n",
              "      <td>[25203361]</td>\n",
              "      <td>0119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>215</th>\n",
              "      <td>452439880874725376</td>\n",
              "      <td>YonB7sDqf9+ts0T3nZcFTCwI+9xx3nCoR7APykRtAE=</td>\n",
              "      <td>YonB7sDqf9+ts0T3nZcFTCwI+9xx3nCoR7APykRtAE=</td>\n",
              "      <td>YonB7sDqf9+ts0T3nZcFTCwI+9xx3nCoR7APykRtAE=</td>\n",
              "      <td>Новосибирск</td>\n",
              "      <td>Добавляю взаимно для общения #followback #rufo...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1673</td>\n",
              "      <td>283</td>\n",
              "      <td>2012-08-03</td>\n",
              "      <td>ru</td>\n",
              "      <td>ru</td>\n",
              "      <td>Volvo планирует продавать 1 млн автомобилей еж...</td>\n",
              "      <td>2014-04-05 13:37</td>\n",
              "      <td>twitterfeed</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>absent</td>\n",
              "      <td>absent</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>['авто']</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                tweetid  ... dataset\n",
              "32   820382233420701697  ...    0119\n",
              "215  452439880874725376  ...    0119\n",
              "\n",
              "[2 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnFj9W6KWaj9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "outputId": "4023076b-a63f-4972-df72-dc56588f6770"
      },
      "source": [
        "# Add the relevant fields from the tweet users to the user dataset\n",
        "\n",
        "extra_tweet_users = extra_tweet_users[['userid', 'user_display_name', 'user_screen_name', 'user_reported_location', 'user_profile_description', 'user_profile_url', 'follower_count', 'following_count', 'account_creation_date', 'account_language', 'dataset']]\n",
        "\n",
        "rus_usersdf_mrg = pd.concat([rus_usersdf_mrg, extra_tweet_users], ignore_index=True)\n",
        "\n",
        "del extra_tweet_users\n",
        "\n",
        "# Verify that the number of users in both datasets are now equal and that those users are in the user dataset\n",
        "print('Number of users in the user dataset')\n",
        "print(len(rus_usersdf_mrg.index))\n",
        "print('Number of users in the tweet dataset')\n",
        "print(len(tweetusers))\n",
        "\n",
        "rus_usersdf_mrg.tail(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of users in the user dataset\n",
            "4861\n",
            "Number of users in the tweet dataset\n",
            "4861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userid</th>\n",
              "      <th>user_display_name</th>\n",
              "      <th>user_screen_name</th>\n",
              "      <th>user_reported_location</th>\n",
              "      <th>user_profile_description</th>\n",
              "      <th>user_profile_url</th>\n",
              "      <th>follower_count</th>\n",
              "      <th>following_count</th>\n",
              "      <th>account_creation_date</th>\n",
              "      <th>account_language</th>\n",
              "      <th>dataset</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4859</th>\n",
              "      <td>iZ328VglWrG25qPym1bifLoiwXD9v1+A3G4WU5AThso=</td>\n",
              "      <td>iZ328VglWrG25qPym1bifLoiwXD9v1+A3G4WU5AThso=</td>\n",
              "      <td>iZ328VglWrG25qPym1bifLoiwXD9v1+A3G4WU5AThso=</td>\n",
              "      <td>United States</td>\n",
              "      <td>No more #HappyHolidays shit!!!\\nIt's #MerryChr...</td>\n",
              "      <td>https://t.co/XFnhCqCWBy</td>\n",
              "      <td>2718</td>\n",
              "      <td>264</td>\n",
              "      <td>2016-06-15</td>\n",
              "      <td>en</td>\n",
              "      <td>0119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4860</th>\n",
              "      <td>YonB7sDqf9+ts0T3nZcFTCwI+9xx3nCoR7APykRtAE=</td>\n",
              "      <td>YonB7sDqf9+ts0T3nZcFTCwI+9xx3nCoR7APykRtAE=</td>\n",
              "      <td>YonB7sDqf9+ts0T3nZcFTCwI+9xx3nCoR7APykRtAE=</td>\n",
              "      <td>Новосибирск</td>\n",
              "      <td>Добавляю взаимно для общения #followback #rufo...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1673</td>\n",
              "      <td>283</td>\n",
              "      <td>2012-08-03</td>\n",
              "      <td>ru</td>\n",
              "      <td>0119</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            userid  ... dataset\n",
              "4859  iZ328VglWrG25qPym1bifLoiwXD9v1+A3G4WU5AThso=  ...    0119\n",
              "4860   YonB7sDqf9+ts0T3nZcFTCwI+9xx3nCoR7APykRtAE=  ...    0119\n",
              "\n",
              "[2 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dqy_jaIuMPV4"
      },
      "source": [
        "Now we know that the users are consistent between the two datasets.\n",
        "\n",
        "Because we cannot conduct lemmatization for preprocessing on non-English languages, we will delete the data for users that have no English language tweets. However, we want to keep the non-English tweets for users that also have tweets in English, as this will be relevant for feature-generation later on. For users that have no English tweets, we will delete their rows in the user data set and all of their tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nShgWvsUQ4Ku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9f6b731-e315-4fb1-f945-c143e32d355f"
      },
      "source": [
        "# Identify the users that have no English tweets\n",
        "\n",
        "# Grab relevant columns\n",
        "twtlang_analysis_df = rus_tweetsdf_mrg[['userid','account_language', 'tweet_language']]\n",
        "usrlang_analysis_df = rus_usersdf_mrg[['userid', 'account_language']]\n",
        "\n",
        "# Tweets that are in English\n",
        "twtdata_tweetlang_en = twtlang_analysis_df[twtlang_analysis_df['tweet_language'] == 'en']\n",
        "\n",
        "# Users with tweets that are in English\n",
        "usertwtcount_bylang = twtlang_analysis_df.groupby(['userid', 'tweet_language']).size().reset_index()\n",
        "users_entweets = usertwtcount_bylang[usertwtcount_bylang['tweet_language'] == 'en']\n",
        "\n",
        "# Users with no English tweets\n",
        "userlang_noen = usertwtcount_bylang[~usertwtcount_bylang.userid.isin(users_entweets.userid)]\n",
        "noen_users = userlang_noen.groupby('userid').size().reset_index()\n",
        "\n",
        "\n",
        "print('The total number of tweets')\n",
        "print(len(twtlang_analysis_df.index))\n",
        "print('\\n')\n",
        "\n",
        "print('The number of tweets in the English language')\n",
        "print(len(twtdata_tweetlang_en.index))\n",
        "print('English tweets account for ' + str((len(twtdata_tweetlang_en.index)/len(twtlang_analysis_df.index))*100) + '% of all tweets')\n",
        "print('\\n')\n",
        "\n",
        "print('The total number of users with English tweets')\n",
        "print(len(users_entweets))\n",
        "print('\\n')\n",
        "\n",
        "print('The total number of users with no English tweets')\n",
        "print(len(noen_users))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The total number of tweets\n",
            "13125555\n",
            "\n",
            "\n",
            "The number of tweets in the English language\n",
            "3768347\n",
            "English tweets account for 28.710001215186708% of all tweets\n",
            "\n",
            "\n",
            "The total number of users with English tweets\n",
            "4158\n",
            "\n",
            "\n",
            "The total number of users with no English tweets\n",
            "694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHdWbalJw12g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5094e7a-0aba-4ed3-c0c5-96713b0b9646"
      },
      "source": [
        "# Delete users with no English tweets from merged user and tweet datasets.\n",
        "rus_usersdf_mrg = rus_usersdf_mrg[~rus_usersdf_mrg.userid.isin(noen_users.userid)]\n",
        "rus_tweetsdf_mrg = rus_tweetsdf_mrg[~rus_tweetsdf_mrg.userid.isin(noen_users.userid)]\n",
        "\n",
        "# Ensure the number of users is still consistent between the two datasets\n",
        "print('Number of users in the user dataset')\n",
        "print(len(rus_usersdf_mrg.index))\n",
        "print('Number of users in the tweet dataset')\n",
        "print(len(rus_tweetsdf_mrg['userid'].unique()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of users in the user dataset\n",
            "4167\n",
            "Number of users in the tweet dataset\n",
            "4167\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRQ2r_sjzBr7"
      },
      "source": [
        "Now that we have gotten rid of the data that we cannot use, we can generate the Bag of Words list from the tweets for every user and append it to their row in the user dataset. We will also generate a Bag of Words and add a row to the tweet dataset. The Bag of Words will only consist of words from English tweets, even though there are tweets in multiple languages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vm7NelrQ9xHS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a5161f4-71ff-4a42-ddd1-097666c88ecb"
      },
      "source": [
        "test_tweetdf = rus_tweetsdf_mrg.iloc[0:3]\n",
        "test_userdf = rus_usersdf_mrg.iloc[0:3]\n",
        "\n",
        "print(test_tweetdf.head())\n",
        "print(test_userdf.head())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "               tweetid  ... dataset\n",
            "0  1290351045160448005  ...    0920\n",
            "1  1268235122131771392  ...    0920\n",
            "2  1283019246503694336  ...    0920\n",
            "\n",
            "[3 rows x 31 columns]\n",
            "                                         userid  ... dataset\n",
            "0  CqW9bECdw2Jjk9DDU7UyE6P59TukYFISNE8J6sN66u4=  ...    0920\n",
            "1    uOrf1TDmM7vP4YEhOJDXORoqvpDlsJt03AyOfhrZo=  ...    0920\n",
            "2   LXW4uuq2JWx4So6ycDFanp4qYQxNvj0ftiuyUe3tZo=  ...    0920\n",
            "\n",
            "[3 rows x 11 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmiXKhB0cKnB"
      },
      "source": [
        "# Create BoW feature in user and tweet datasets\n",
        "\n",
        "tweetProcessing.preprocess_frame(rus_tweetsdf_mrg, rus_usersdf_mrg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg_lidljODXa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e62f121f-560f-4da4-c7a6-4f3ab7c3692e"
      },
      "source": [
        "# Verify BoW feature was created\n",
        "\n",
        "print(rus_tweetsdf_mrg.head(10)['tweet_text'])\n",
        "print(rus_tweetsdf_mrg.head(10)['BoW'])\n",
        "print(rus_usersdf_mrg['BoW'].head(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0    RT @Claudia90291: Never did I ever:\\n\\nsee Sch...\n",
            "1    RT @mlk_institute: \"It is high time that we re...\n",
            "2    RT @davidsirota: Fear: Trump wins reelection.\\...\n",
            "3    RT @curbstompchloe: lemme introduce the tl to ...\n",
            "4    RT @papichulomin: Oh you “love Obama”? Name 7 ...\n",
            "5    RT @MelanieMoore: The police officers walking ...\n",
            "6    RT @LeftistFun: Do you think that people are f...\n",
            "7    RT @JoyAnnReid: What year ... what century are...\n",
            "8    RT @ABC: BREAKING: All four responding officer...\n",
            "9    RT @norvergence: #WaterSecurity in #Jordan is ...\n",
            "Name: tweet_text, dtype: object\n",
            "0                   [schumer, fight, hard, healthcare]\n",
            "1    [high, time, retire, white, racist, congress, ...\n",
            "2    [fear, trump, win, reelection, fear, democrati...\n",
            "3            [lemme, introduce, tl, favorite, graphic]\n",
            "4                   [oh, love, obama, country, bombed]\n",
            "5    [police, officer, walk, charge, commit, murder...\n",
            "6            [think, people, fundamentally, good, bad]\n",
            "7                                      [year, century]\n",
            "8    [breaking, respond, officer, involve, death, b...\n",
            "9    [watersecurity, jordan, crucial, maintaining, ...\n",
            "Name: BoW, dtype: object\n",
            "0     [schumer, fight, hard, healthcare, breaking, r...\n",
            "1     [watersecurity, jordan, crucial, maintaining, ...\n",
            "2     [freedom, press, acknowledge, civilized, world...\n",
            "3     [high, time, retire, white, racist, congress, ...\n",
            "6     [set, twitter, myfirsttweet, man, buys, new, h...\n",
            "7                        [big, russian, boss, brazzers]\n",
            "8     [yesterday, great, honor, host, annual, americ...\n",
            "10    [post, new, photo, facebook, post, new, photo,...\n",
            "11    [vous, aimez, alors, venez, like, come, art, f...\n",
            "15    [tbt, neville, jacobs, trophywife, body, lava,...\n",
            "Name: BoW, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfMU4EX437Ji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4ed239-c5d1-4a11-cc3f-321e90430e8c"
      },
      "source": [
        "# Convert the BoW into a single, comprehensive list to use for user query\n",
        "# Evaluate differences between different BoW\n",
        "\n",
        "BoW_list = []\n",
        "BoW_prelist = rus_usersdf_mrg.BoW.tolist()\n",
        "BoW_list = [item for sublist in BoW_prelist for item in sublist]\n",
        "BoW_list = pd.Series(BoW_list)\n",
        "\n",
        "# Determine how many unique words there are and the most frequently used words\n",
        "BoW_unique = BoW_list.unique()\n",
        "BoW_counts = BoW_list.value_counts()\n",
        "\n",
        "print('There are a total of ' + str(len(BoW_list)) + ' preprocessed words in the BoW dataset')\n",
        "print('There are a total of ' + str(len(BoW_unique)) + ' unique preprocessed words in the BoW dataset\\n')\n",
        "print('The top 50 most used words in the dataset:')\n",
        "print(BoW_counts.head(50))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are a total of 25741529 preprocessed words in the BoW dataset\n",
            "There are a total of 563378 unique preprocessed words in the BoW dataset\n",
            "\n",
            "The top 50 most used words in the dataset:\n",
            "news         282821\n",
            "trump        224713\n",
            "amp          128947\n",
            "new          116600\n",
            "people       108153\n",
            "like         104972\n",
            "sport        101087\n",
            "love          96441\n",
            "man           96100\n",
            "obama         95977\n",
            "politics      85767\n",
            "know          80440\n",
            "police        79699\n",
            "want          78363\n",
            "time          76075\n",
            "world         74676\n",
            "http          73831\n",
            "year          72708\n",
            "day           71152\n",
            "today         70433\n",
            "good          69146\n",
            "need          67363\n",
            "life          66313\n",
            "woman         64269\n",
            "america       62622\n",
            "hillary       62560\n",
            "local         62192\n",
            "win           60015\n",
            "think         58676\n",
            "president     58424\n",
            "black         56718\n",
            "kill          54794\n",
            "islam         54523\n",
            "right         53456\n",
            "look          53442\n",
            "clinton       53322\n",
            "state         48215\n",
            "come          46995\n",
            "white         46585\n",
            "great         46501\n",
            "let           45758\n",
            "video         45614\n",
            "thing         44812\n",
            "workout       44652\n",
            "maga          40437\n",
            "live          40385\n",
            "vote          39528\n",
            "business      39316\n",
            "work          39226\n",
            "way           38953\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx_zTzbsKZRY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c3802d0-1b8d-4530-89e3-a59e2e4444fb"
      },
      "source": [
        "# Analyze the words by dataset\n",
        "\n",
        "BoW_df = rus_usersdf_mrg[['BoW', 'dataset']]\n",
        "\n",
        "BoW_df_0920 = BoW_df[BoW_df['dataset'] == '0920']\n",
        "BoW_df_0520 = BoW_df[BoW_df['dataset'] == '0520']\n",
        "BoW_df_0619 = BoW_df[BoW_df['dataset'] == '0619']\n",
        "BoW_df_0119 = BoW_df[BoW_df['dataset'] == '0119']\n",
        "BoW_df_01018 = BoW_df[BoW_df['dataset'] == '1018']\n",
        "\n",
        "print('Number of rows and columns in each dataset. The number of rows correspond to the number of users\\n')\n",
        "print('0920 Dataset')\n",
        "print(BoW_df_0920.shape)\n",
        "print('\\n0520 Dataset')\n",
        "print(BoW_df_0520.shape)\n",
        "print('\\n0619 Dataset')\n",
        "print(BoW_df_0619.shape)\n",
        "print('\\n0119 Dataset')\n",
        "print(BoW_df_0119.shape)\n",
        "print('\\n1018 Dataset')\n",
        "print(BoW_df_1018.shape)\n",
        "\n",
        "# Take words from BoW\n",
        "BoW_list_0920 = []\n",
        "BoW_prelist_0920 = BoW_df_0920.BoW.tolist()\n",
        "BoW_list_0920 = [item for sublist in BoW_prelist_0920 for item in sublist]\n",
        "BoW_list_0920 = pd.Series(BoW_list_0920)\n",
        "\n",
        "BoW_list_0520 = []\n",
        "BoW_prelist_0520 = BoW_df_0520.BoW.tolist()\n",
        "BoW_list_0520 = [item for sublist in BoW_prelist_0520 for item in sublist]\n",
        "BoW_list_0520 = pd.Series(BoW_list_0520)\n",
        "\n",
        "BoW_list_0619 = []\n",
        "BoW_prelist_0619 = BoW_df_0619.BoW.tolist()\n",
        "BoW_list_0619 = [item for sublist in BoW_prelist_0619 for item in sublist]\n",
        "BoW_list_0619 = pd.Series(BoW_list_0619)\n",
        "\n",
        "BoW_list_0119 = []\n",
        "BoW_prelist_0119 = BoW_df_0119.BoW.tolist()\n",
        "BoW_list_0119 = [item for sublist in BoW_prelist_0119 for item in sublist]\n",
        "BoW_list_0119 = pd.Series(BoW_list_0119)\n",
        "\n",
        "BoW_list_1018 = []\n",
        "BoW_prelist_1018 = BoW_df_1018.BoW.tolist()\n",
        "BoW_list_1018 = [item for sublist in BoW_prelist_1018 for item in sublist]\n",
        "BoW_list_1018 = pd.Series(BoW_list_1018)\n",
        "\n",
        "BoW_list_0920_unique = BoW_list_0920.unique()\n",
        "BoW_list_0920_counts = BoW_list_0920.value_counts()\n",
        "\n",
        "BoW_list_0520_unique = BoW_list_0520.unique()\n",
        "BoW_list_0520_counts = BoW_list_0520.value_counts()\n",
        "\n",
        "BoW_list_0619_unique = BoW_list_0619.unique()\n",
        "BoW_list_0619_counts = BoW_list_0619.value_counts()\n",
        "\n",
        "BoW_list_0119_unique = BoW_list_0119.unique()\n",
        "BoW_list_0119_counts = BoW_list_0119.value_counts()\n",
        "\n",
        "BoW_list_1018_unique = BoW_list_1018.unique()\n",
        "BoW_list_1018_counts = BoW_list_1018.value_counts()\n",
        "\n",
        "\n",
        "print('The top most used words and number of unique words by dataset\\n')\n",
        "\n",
        "print('0920 Dataset')\n",
        "print(len(BoW_list_0920_unique), ' unique words')\n",
        "print(BoW_list_0920_counts.head(50))\n",
        "print('\\n')\n",
        "\n",
        "print('0520 Dataset')\n",
        "print(len(BoW_list_0520_unique), ' unique words')\n",
        "print(BoW_list_0520_counts.head(50))\n",
        "print('\\n')\n",
        "\n",
        "print('0619 Dataset')\n",
        "print(len(BoW_list_0619_unique), ' unique words')\n",
        "print(BoW_list_0619_counts.head(50))\n",
        "print('\\n')\n",
        "\n",
        "print('0119 Dataset')\n",
        "print(len(BoW_list_0119_unique), ' unique words')\n",
        "print(BoW_list_0119_counts.head(50))\n",
        "print('\\n')\n",
        "\n",
        "print('1018 Dataset')\n",
        "print(len(BoW_list_1018_unique), ' unique words')\n",
        "print(BoW_list_1018_counts.head(50))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of rows and columns in each dataset. The number of rows correspond to the number of users\n",
            "\n",
            "0920 Dataset\n",
            "(4, 2)\n",
            "\n",
            "0520 Dataset\n",
            "(730, 2)\n",
            "\n",
            "0619 Dataset\n",
            "(2, 2)\n",
            "\n",
            "0119 Dataset\n",
            "(349, 2)\n",
            "\n",
            "1018 Dataset\n",
            "(3082, 2)\n",
            "The top most used words and number of unique words by dataset\n",
            "\n",
            "0920 Dataset\n",
            "2934  unique words\n",
            "message        124\n",
            "direct         122\n",
            "new            100\n",
            "war             85\n",
            "like            81\n",
            "government      74\n",
            "people          72\n",
            "non             68\n",
            "america         67\n",
            "trump           67\n",
            "hello           66\n",
            "send            65\n",
            "disable         65\n",
            "following       65\n",
            "discuss         63\n",
            "proposal        62\n",
            "right           61\n",
            "police          52\n",
            "coronavirus     51\n",
            "military        50\n",
            "report          48\n",
            "crisis          47\n",
            "covid           46\n",
            "american        43\n",
            "yemen           42\n",
            "country         41\n",
            "pandemic        40\n",
            "saudi           40\n",
            "history         38\n",
            "year            37\n",
            "way             36\n",
            "piece           36\n",
            "continue        36\n",
            "nuclear         35\n",
            "human           35\n",
            "uk              34\n",
            "security        34\n",
            "corporate       34\n",
            "white           32\n",
            "business        31\n",
            "world           30\n",
            "united          29\n",
            "president       29\n",
            "million         27\n",
            "arabia          26\n",
            "crime           26\n",
            "china           26\n",
            "change          25\n",
            "news            25\n",
            "western         25\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "0520 Dataset\n",
            "24012  unique words\n",
            "girl              2493\n",
            "sex               2448\n",
            "nude              2017\n",
            "porn              1752\n",
            "new               1567\n",
            "naked             1357\n",
            "free              1286\n",
            "follower          1200\n",
            "teen              1142\n",
            "video             1108\n",
            "pic               1050\n",
            "fuck              1047\n",
            "woman              958\n",
            "like               942\n",
            "football           926\n",
            "good               910\n",
            "week               908\n",
            "forinnovations     829\n",
            "day                823\n",
            "sexy               795\n",
            "stats              775\n",
            "photo              759\n",
            "lol                730\n",
            "friendship         694\n",
            "love               676\n",
            "russia             655\n",
            "know               605\n",
            "amp                600\n",
            "people             599\n",
            "picture            598\n",
            "unfollowers        575\n",
            "year               563\n",
            "hot                551\n",
            "pussy              550\n",
            "xxx                547\n",
            "time               544\n",
            "happy              535\n",
            "want               534\n",
            "need               521\n",
            "world              488\n",
            "look               475\n",
            "movie              466\n",
            "today              464\n",
            "morning            441\n",
            "porno              434\n",
            "follow             415\n",
            "life               414\n",
            "twitter            412\n",
            "big                401\n",
            "moscow             393\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "0619 Dataset\n",
            "23  unique words\n",
            "mueller      3\n",
            "russian      3\n",
            "concord      1\n",
            "special      1\n",
            "file         1\n",
            "picture      1\n",
            "database     1\n",
            "view         1\n",
            "hack         1\n",
            "collusion    1\n",
            "case         1\n",
            "probe        1\n",
            "access       1\n",
            "enjoy        1\n",
            "reading      1\n",
            "counsel      1\n",
            "ira          1\n",
            "troll        1\n",
            "ve           1\n",
            "server       1\n",
            "info         1\n",
            "help         1\n",
            "llc          1\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "0119 Dataset\n",
            "129577  unique words\n",
            "trump        95223\n",
            "amp          51513\n",
            "islam        49302\n",
            "today        39431\n",
            "obama        36614\n",
            "http         35917\n",
            "hillary      30507\n",
            "america      30206\n",
            "lesson       29520\n",
            "president    24060\n",
            "people       24054\n",
            "maga         23556\n",
            "clinton      22417\n",
            "new          19608\n",
            "like         19233\n",
            "know         19054\n",
            "muslim       18196\n",
            "want         17844\n",
            "need         17039\n",
            "time         16223\n",
            "isis         15408\n",
            "news         14640\n",
            "year         14445\n",
            "vote         14386\n",
            "great        13429\n",
            "white        13039\n",
            "think        13000\n",
            "right        12640\n",
            "day          12574\n",
            "woman        12431\n",
            "tcot         12300\n",
            "state        12276\n",
            "american     12270\n",
            "breaking     12085\n",
            "win          12046\n",
            "good         12012\n",
            "fbi          11871\n",
            "donald       11840\n",
            "attack       11840\n",
            "watch        11439\n",
            "muslims      11266\n",
            "kill         10961\n",
            "video        10708\n",
            "black        10704\n",
            "country      10646\n",
            "qanon        10550\n",
            "support      10401\n",
            "love         10289\n",
            "police       10204\n",
            "stop         10140\n",
            "dtype: int64\n",
            "\n",
            "\n",
            "1018 Dataset\n",
            "513132  unique words\n",
            "news         268034\n",
            "trump        129385\n",
            "sport        100395\n",
            "new           95325\n",
            "man           85925\n",
            "love          85470\n",
            "like          84716\n",
            "politics      83631\n",
            "people        83428\n",
            "amp           76821\n",
            "police        69393\n",
            "world         65613\n",
            "local         60811\n",
            "know          60775\n",
            "want          59974\n",
            "obama         59315\n",
            "time          59297\n",
            "life          58791\n",
            "day           57734\n",
            "year          57663\n",
            "good          56212\n",
            "woman         50869\n",
            "need          49782\n",
            "win           47799\n",
            "black         45682\n",
            "think         45312\n",
            "workout       44604\n",
            "kill          43715\n",
            "look          42971\n",
            "right         40474\n",
            "thing         37852\n",
            "http          37764\n",
            "come          36981\n",
            "business      36750\n",
            "state         35859\n",
            "chicago       35641\n",
            "let           35411\n",
            "president     34257\n",
            "health        34028\n",
            "video         33793\n",
            "white         33382\n",
            "live          33061\n",
            "great         32719\n",
            "america       32302\n",
            "hillary       32034\n",
            "way           31779\n",
            "break         30974\n",
            "clinton       30884\n",
            "work          30613\n",
            "today         30530\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJflDzWuLquD"
      },
      "source": [
        "We will save further analysis for the Exploratory Data Analysis step later, this was just to ensure that the data processed so far will work for our intended purposes. While some of the datasets are not as large as others, they seem to all generally be about similar topics so it should not matter and we can keep all of the datasets.\n",
        "\n",
        "Now that we have formatted and generated our data, we will save it to a csv and continue to the next step in another notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiGV9pOqgGjh"
      },
      "source": [
        "# Send user data with BoW to csv for Step 3\n",
        "\n",
        "rus_usersdf_mrg.to_csv('/mypath/Step 3 - Feature Generation/Input_Data_Step3/rus_users_bow.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0lsAogiFOcF"
      },
      "source": [
        "# Store BoW List in a csv to reference for the legitimate tweet dataset query in Step 2\n",
        "\n",
        "BoW_list.to_csv('/mypath/Step 2 - Query Legitimate Tweets Dataset/Input_Data_Step2/BoW_list.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR6uFJYSNKYN"
      },
      "source": [
        "# Delete some variables to make space in RAM\n",
        "\n",
        "del BoW_list, rus_usersdf_mrg, BoW_list_0920, BoW_list_0520, BoW_list_0619, BoW_list_0119, BoW_list_1018"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TNtU5tCoFHr"
      },
      "source": [
        "# The tweet dataset is pretty large, so we will have to save it as multiple csv files for Step 3\n",
        "\n",
        "# Save 0920 dataset\n",
        "rus_tweetsdf_mrg[rus_tweetsdf_mrg['dataset'] == '0920'].to_csv('/mypath/Step 3 - Feature Generation/Input_Data_Step3/rus_tweets_0920.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5aZd6taTQLc"
      },
      "source": [
        "# Save 0520 dataset\n",
        "\n",
        "rus_tweetsdf_mrg[rus_tweetsdf_mrg['dataset'] == '0520'].to_csv('/mypath/Step 3 - Feature Generation/Input_Data_Step3/rus_tweets_0520.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5SmDZA1TPR2"
      },
      "source": [
        "# Save 0119 dataset\n",
        "\n",
        "rus_tweetsdf_mrg[rus_tweetsdf_mrg['dataset'] == '0119'].to_csv('/mypath/Step 3 - Feature Generation/Input_Data_Step3/rus_tweets_0119.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2iuk8x0TP7b"
      },
      "source": [
        "# Save 0619 dataset\n",
        "\n",
        "rus_tweetsdf_mrg[rus_tweetsdf_mrg['dataset'] == '0619'].to_csv('/mypath/Step 3 - Feature Generation/Input_Data_Step3/rus_tweets_0619.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpOGJqn2TPr-"
      },
      "source": [
        "# Save 1018 dataset\n",
        "\n",
        "rus_tweetsdf_mrg[rus_tweetsdf_mrg['dataset'] == '1018'].to_csv('/mypath/Step 3 - Feature Generation/Input_Data_Step3/rus_tweets_1018.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh5nftYLzvGO"
      },
      "source": [
        "Now that the Russian dataset has been preprocessed and formatted and we have a list of search terms, we will conduct our query via the Twitter API to generate the legitimate user tweet dataset."
      ]
    }
  ]
}